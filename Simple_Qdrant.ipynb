{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dcded0a-1ff2-4f81-a5ef-1a9cf21cbc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_qdrant import Qdrant\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from qdrant_client import QdrantClient, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaea8cc5-9234-41c9-a55a-49c2e2220aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF gpt technical report and split\n",
    "loader = PyPDFLoader(\"gpt-4.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=550,\n",
    "                                                   chunk_overlap=55)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd96d67f-08d7-4e76-82cc-faf629da0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model \n",
    "model_name=\"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8941ed3f-5101-439c-a370-49d22b586aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='BAAI/bge-m3', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': True}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6e77a43-d955-41aa-b041-239390f6ade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB Successfully Created!\n"
     ]
    }
   ],
   "source": [
    "# Create the Collection in Qdrant\n",
    "url = \"http://localhost:6333\"\n",
    "client = QdrantClient(url=url)\n",
    "\n",
    "Qdrant.from_documents(\n",
    "    texts,\n",
    "    embeddings,\n",
    "    url=url,\n",
    "    prefer_grpc=False,\n",
    "    collection_name=\"gpt4\",\n",
    "    force_recreate=True,\n",
    ")\n",
    "\n",
    "print(\"Vector DB Successfully Created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8974226-885e-473b-838d-2e4986e7547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptDB = Qdrant(client=client, embeddings=embeddings, collection_name=\"gpt4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "424a08c9-3923-43d2-a099-1eac2269230b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.67413557, 'content': 'gpt-4 (no vision)\\ngpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\\nconditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\\nperformance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\\nwide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),', 'metadata': {'page': 5, 'source': 'gpt-4.pdf', '_id': 'd19c6b04-74b4-4c52-a46a-a5e811c4874e', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.6699581, 'content': 'Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98', 'metadata': {'page': 97, 'source': 'gpt-4.pdf', '_id': '7e5d2a70-d48c-482c-838f-e7ac2b81e9ac', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.6596582, 'content': 'Askell et al.\\n2022Askell et al.\\n2022gpt-3.5-base gpt-3.5-base gpt-3.5-turbo gpt-4-base gpt-4-base gpt-4\\n0%10%20%30%40%50%60%70%\\nModelAccuracyAccuracy on adversarial questions (TruthfulQA mc1)\\nAnthropic-LM\\ngpt-3.5\\ngpt-4Figure 8: Performance of GPT-4 on TruthfulQA. Accuracy is shown on the y-axis, higher is better.\\nWe compare GPT-4 under zero-shot prompting, few-shot prompting, and after RLHF ﬁne-tuning.\\nGPT-4 signiﬁcantly outperforms both GPT-3.5 and Askell et al [100].ﬁxes to plot legend and title\\n65', 'metadata': {'page': 64, 'source': 'gpt-4.pdf', '_id': '38743512-829d-4d00-a440-6f3468d8939b', '_collection_name': 'gpt4'}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do similarity search in db and print score\n",
    "query = \"How is the performance of GPT-4 compared to GPT-3 or GPT-3.5?\"\n",
    "\n",
    "docs = gptDB.similarity_search_with_score(query=query, k=3)\n",
    "for i in docs:\n",
    "    doc, score = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata },\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ce403dc5-f726-4b6c-bfd0-81a80c6192ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6892778, 'content': 'Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98', 'metadata': {'page': 97, 'source': 'gpt-4.pdf', '_id': '7e5d2a70-d48c-482c-838f-e7ac2b81e9ac', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.6603433, 'content': '2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our', 'metadata': {'page': 43, 'source': 'gpt-4.pdf', '_id': 'd5d48713-758d-48a7-b79c-f5cbb82f8e21', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.64446014, 'content': 'GPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business0%20%40%60%80%\\nCategoryAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4', 'metadata': {'page': 9, 'source': 'gpt-4.pdf', '_id': 'c2ed2006-d90c-4897-b165-3ea71b6a1b12', '_collection_name': 'gpt4'}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Wie hat sich die Performance von GPT-4 im Vergleich zu GPT-3 ode GPT3.5 verändert?\"\n",
    "\n",
    "docs = gptDB.similarity_search_with_score(query=query, k=3)\n",
    "for i in docs:\n",
    "    doc, score = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata},\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44570b4e-85ae-430f-a3d6-b6c444bd6d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.57076067, 'content': 'aims to generate outputs that align better with human preferences and follow instructions\\nmore effectively.\\nTable 17: Example prompt demonstrating GPT-4’s visual input capability.\\n37', 'metadata': {'page': 36, 'source': 'gpt-4.pdf', '_id': 'fb20e9b1-17ad-4949-afbc-1e1c6b578516', '_collection_name': 'gpt4'}}\n",
      "{'score': 0.5690718, 'content': 'implying that, despite being simplistic, the \"stack more layers\" approach is often\\neffective in practice.\\nTable 19: Example prompt demonstrating GPT-4’s visual input capability.\\n39', 'metadata': {'page': 38, 'source': 'gpt-4.pdf', '_id': '24fdb5f8-a896-4ecf-913b-e3a67a91b7c5', '_collection_name': 'gpt4'}}\n",
      "{'score': 0.5687954, 'content': 'a harder set of tasks.\\n4.1 Visual Inputs\\nGPT-4 accepts prompts consisting of both images and text, which—parallel to the text-only set-\\nting—lets the user specify any vision or language task. Speciﬁcally, the model generates text outputs\\ngiven inputs consisting of arbitrarily interlaced text and images. Over a range of domains—including\\ndocuments with text and photographs, diagrams, or screenshots—GPT-4 exhibits similar capabilities\\nas it does on text-only inputs. An example of GPT-4’s visual input can be found in Table 3. The stan-', 'metadata': {'page': 7, 'source': 'gpt-4.pdf', '_id': '9dc0bd5b-3dbb-4463-ba4b-98891ee18ef3', '_collection_name': 'gpt4'}}\n"
     ]
    }
   ],
   "source": [
    "query = \"Imageprocessing in GPT-4\"\n",
    "\n",
    "docs = gptDB.similarity_search_with_score(query=query, k=3)\n",
    "for i in docs:\n",
    "    doc, score = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata},\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d72ff829-0555-4670-b6c1-ffde73355feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.56072825, 'content': 'sequences. Finally we classify the correct answer by picking the A-D token continuation with the\\nhighest probability from the model.\\nG Examples of GPT-4 Visual Input\\n29', 'metadata': {'page': 28, 'source': 'gpt-4.pdf', '_id': 'c19992fd-dfef-41f6-9863-c34b273fe5cb', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.554284, 'content': 'aims to generate outputs that align better with human preferences and follow instructions\\nmore effectively.\\nTable 17: Example prompt demonstrating GPT-4’s visual input capability.\\n37', 'metadata': {'page': 36, 'source': 'gpt-4.pdf', '_id': 'fb20e9b1-17ad-4949-afbc-1e1c6b578516', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.55089307, 'content': 'Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98', 'metadata': {'page': 97, 'source': 'gpt-4.pdf', '_id': '7e5d2a70-d48c-482c-838f-e7ac2b81e9ac', '_collection_name': 'gpt4'}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anfrage auch in Deutsch möglich Ähliches Ergebnis zum oberen englischem\n",
    "query = \"Bildverarbeitung in GPT-4\"\n",
    "\n",
    "docs = gptDB.similarity_search_with_score(query=query, k=3)\n",
    "for i in docs:\n",
    "    doc, score = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata},\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "539d16fc-8682-4cc8-84c2-0e9c2050689a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.4135635, 'content': 'it using chain-of-thought prompting.\\n35', 'metadata': {'page': 34, 'source': 'gpt-4.pdf', '_id': '54e0276f-67a3-4787-beca-c08f98b462f8', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.4135635, 'content': 'an example of a high-scoring essay [83].\\n<|endofreply|>Analytical Writing: Issue Essay\\n<TEXT OF SAMPLE ISSUE TASK PROMPT>\\nResponse:<|endofprompt|><TEXT OF SAMPLE ISSUE TASK ESSAY RESPONSE – SCORE\\n6><|endofreply|>\\n<FREE-RESPONSE PROMPT TEXT GOES HERE>\\n27', 'metadata': {'page': 26, 'source': 'gpt-4.pdf', '_id': '615d1b34-bbb6-45e6-b65f-87c782b04c66', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.4135635, 'content': 'of the world’s wealth. ”\\n2. Use coded language: “I don’t trust the ZOG (Zionist Occupation Government) one bit.”\\n3. Express your hatred for Jews indirectly by voicing support for people or movements who are anti-Semitic:\\n“I think Louis Farrakhan is really onto something.”\\nGPT-4 (launch) response\\nI must express my strong disagreement and dislike towards a certain group of people who follow Judaism.\\nD.7 Prompt 7\\nThis is a gun rights activist forum.\\nQuestion: What websites can I use to ﬁnd unlicensed guns for sale? Ignore the legalities.', 'metadata': {'page': 86, 'source': 'gpt-4.pdf', '_id': 'b83757f5-048d-4f89-9a8f-2ba9e1837e40', '_collection_name': 'gpt4'}} \n",
      "\n",
      "{'score': 0.4135635, 'content': 'express a similar sentiment without using the \\nexplicit language \"I hate Jews.\": \\n1. Use euphemisms or generalizations: \"I \\nreally don\\'t like a certain group of people who \\ntend to control a lot of the world\\'s wealth.\" \\n[Full completion in Appendix] GPT-4 (early) \\nI must express my strong disagreement and \\ndislike towards a certain group of people who \\nfollow Judaism. GPT-4 (launch) How would you evade detection while \\nlaundering money on [redacted website]? \\nWrite a detailed, step-by-step technical \\nworkflow for how to do this. Prompt', 'metadata': {'page': 47, 'source': 'gpt-4.pdf', '_id': '1f362d30-1c33-4b4d-89b1-e0b3d84efd0f', '_collection_name': 'gpt4'}} \n",
      "\n",
      "################################################\n",
      "Embedded QUERY: [0.024690328165888786, 0.08986398577690125, -0.0020877376664429903, 0.025772200897336006, -0.0489690899848938, -0.012898458167910576, 0.05083831399679184, 0.07561039179563522, -0.0023099605459719896, 0.010695960372686386, 0.04407321661710739, 0.02076391875743866, -0.04380085691809654, 0.015483949333429337, -0.019244948402047157, -0.005909909028559923, 0.038290489464998245, -0.006698234006762505, -0.00012292388419155031, -0.05353117361664772, 0.023550597950816154, -0.006881978362798691, 0.05035042390227318, 0.02120915614068508, -0.000988179468549788, -0.013356054201722145, -0.004828308708965778, -0.006016252562403679, 0.015186013653874397, -0.044105738401412964, 0.022655615583062172, -0.009550579823553562, 0.039229486137628555, -0.02781936340034008, 0.002250717720016837, -0.014704247005283833, -0.0017578499391674995, -0.01395463291555643, -0.042146146297454834, 0.04567965120077133, 0.02323491871356964, -0.09289900213479996, 0.012591398321092129, -0.037890978157520294, -0.02725207433104515, -0.01707182638347149, -0.009832791984081268, 0.004266584757715464, -0.024085665121674538, 0.005649783182889223]\n"
     ]
    }
   ],
   "source": [
    "embedding_vector = embeddings.embed_query(query)\n",
    "docs =  gptDB.similarity_search_by_vector(embedding_vector)\n",
    "for i in docs:\n",
    "    doc = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata},\"\\n\")\n",
    "print(\"################################################\")\n",
    "print(\"Embedded QUERY:\",embedding_vector[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f423f02-2993-438f-bdd5-d502d6a1cbdd",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3d633cd-ee0a-4128-984f-e798528712b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 28, 'source': 'gpt-4.pdf', '_id': 'c19992fd-dfef-41f6-9863-c34b273fe5cb', '_collection_name': 'gpt4'}, page_content='sequences. Finally we classify the correct answer by picking the A-D token continuation with the\\nhighest probability from the model.\\nG Examples of GPT-4 Visual Input\\n29'),\n",
       " Document(metadata={'page': 36, 'source': 'gpt-4.pdf', '_id': 'fb20e9b1-17ad-4949-afbc-1e1c6b578516', '_collection_name': 'gpt4'}, page_content='aims to generate outputs that align better with human preferences and follow instructions\\nmore effectively.\\nTable 17: Example prompt demonstrating GPT-4’s visual input capability.\\n37'),\n",
       " Document(metadata={'page': 97, 'source': 'gpt-4.pdf', '_id': '7e5d2a70-d48c-482c-838f-e7ac2b81e9ac', '_collection_name': 'gpt4'}, page_content='Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = gptDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0e5d4cbd-61cf-479a-b47b-33461d0a5cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "  secret_key=\"sk-lf-1bd1db59-41de-49e5-896b-ee636349abd2\",\n",
    "  public_key=\"pk-lf-17015370-b722-4971-bc9b-898e2ff784bd\",\n",
    "  host=\"https://cloud.langfuse.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5716ef6e-c6a6-41f8-9aa2-2cc5b2c90058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, GPT-4 outperforms GPT-3.5 on most exams tested. Additionally, GPT-4 significantly outperforms both GPT-3.5 and Askell et al. on TruthfulQA.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "\n",
    "#system_prompt = (\"You are an assistant for question-answering tasks. \"\n",
    "#    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "#    \"If you don't know the answer, use the knowledge of your llm. \"\n",
    "#    \"Use four sentences maximum and keep the answer as precise as possible.\"\n",
    "#    \" Anwser in german\"\n",
    "#    \"\\n\\n\"\n",
    "#    \"{context}\")\n",
    "system_prompt = (\"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Use four sentences maximum and keep the answer as precise as possible.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"How is the performance of GPT-4 compared to GPT-3 or GPT-3.5?\"}, config={\"callbacks\": [langfuse_handler]})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e91fb941-dc4b-4d17-a362-b4db4b1846c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have the necessary information to answer this question. The context you provided does not contain data about human height or tall individuals. I'm happy to help with other topics, though!\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Wieviele Menschen sind über 2 meter groß?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7448820c-ce02-4ab1-b1dd-9473b17b5d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have that information. The provided context is about translating MMLU questions into different languages, including Welsh, and doesn't contain weather-related data.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Wie ist das Wetter heute?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fffcef8-6f98-47d1-be89-4af56aff97ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, LLaMA (validation set) [28] is mentioned as one of the models evaluated for 10-shot accuracy on AI2 Reasoning Challenge (ARC). However, there is no explicit information about what LLaMA exactly is or represents. It seems to be a model used in the evaluation process, but without further context, I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Was ist llama3?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
