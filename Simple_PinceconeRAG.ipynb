{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fe1c8d7-a662-4167-ae6d-5f0959402f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"]=\"7c6028ed-8d2a-4003-8e7f-fca445aad9a1\"\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e5d671-c8be-4907-9231-08c11b5e86c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1814e35357a8441da0e0f146c2546dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chringo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Chringo\\.cache\\huggingface\\hub\\models--BAAI--bge-m3. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c412cea6562e4909b6c58cf7b99f4e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2875e5ad56f84b7fa8c52b3f13902d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/15.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e526f6394f4b434b8da67f9394bc61df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chringo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826d47540b524b26a3c0b2c5cace5abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad36d85c17ab44d28134aac44ae815d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35d02295e0e452a88d981d963ee0ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4c0f4a3605462e93401c281fdf4f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4763fa6c5ff34beebbcabd6437fe85df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472ef641d82f4d27a4541bc3dee32493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5ca23d6ab24caab3b75132ce7961f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the embedding model \n",
    "model_name=\"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd59039-66e3-4431-a309-7434247db735",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'langchain-retrieval-augmentation-fast'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b4fd8a-143c-4b71-aa0c-04aea91d7de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='BAAI/bge-m3', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': False}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64348963-a050-4279-ad47-c5247719507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF gpt technical report and split\n",
    "loader = PyPDFLoader(\"gpt-4.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                                   chunk_overlap=55)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb0d1fbd-5f53-47b2-a1ca-e2888493c174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d905f814-b274-4c3c-989b-fa7c5ad3a3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f16b2281-0eb7-4932-b844-c6defa4322fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Pinecone index and insert the chunked docs as contents\n",
    "docsearch = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5d4575-8369-49e6-917e-4c0d7aab6575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61fe273d-20ad-4185-85a7-dd11cc19da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 671}},\n",
      " 'total_vector_count': 671}\n"
     ]
    }
   ],
   "source": [
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cbc562bf-00a4-4cc6-a153-9686e4693854",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Was macht GPT4 besser als GPT3 und GPT3.5=\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7567f4a4-28fe-4390-ac71-1cdd986f5fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='GPT-4 signiﬁcantly outperforms both GPT-3.5 and Askell et al [100].ﬁxes to plot legend and title\\n65', metadata={'page': 64.0, 'source': 'gpt-4.pdf'}),\n",
       "  0.673246682),\n",
       " (Document(page_content='gpt-4 (no vision)\\ngpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\\nconditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\\nperformance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very', metadata={'page': 5.0, 'source': 'gpt-4.pdf'}),\n",
       "  0.656640768),\n",
       " (Document(page_content='GPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business0%20%40%60%80%\\nCategoryAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4', metadata={'page': 9.0, 'source': 'gpt-4.pdf'}),\n",
       "  0.624052107),\n",
       " (Document(page_content='Based on our general capability evaluations, we expect GPT-4 to be better than GPT-3 at producing\\nrealistic, targeted content. As such, there is risk of GPT-4 being used for generating content that is\\nintended to mislead.[50]\\nEmpirical evidence suggests that earlier language models could also be useful for generating\\ncontent that is misleading, but persuasive.[ 51] For example, researchers found that GPT-3 was', metadata={'page': 49.0, 'source': 'gpt-4.pdf'}),\n",
       "  0.623605072)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.similarity_search_with_score(query, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e095aaf-3d5b-44a8-b4bb-abd83f04d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "17a2ed7d-798b-4286-9f35-99719594e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='GPT-4 signiﬁcantly outperforms both GPT-3.5 and Askell et al [100].ﬁxes to plot legend and title\\n65', metadata={'page': 64.0, 'source': 'gpt-4.pdf'}),\n",
       " Document(page_content='gpt-4 (no vision)\\ngpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\\nconditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\\nperformance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very', metadata={'page': 5.0, 'source': 'gpt-4.pdf'}),\n",
       " Document(page_content='GPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business0%20%40%60%80%\\nCategoryAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4', metadata={'page': 9.0, 'source': 'gpt-4.pdf'})]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "87c5e49f-1077-4dce-bfc0-6d045761caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, GPT-4 outperforms both GPT-3.5 and Askell et al on most exams tested, significantly reduces hallucinations compared to previous GPT-3.5 models, and scores 19 percentage points higher than the latest GPT-3.5 on internal factuality evaluations.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# LangChain supports many other chat models. Here, we're using Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# supports many more optional parameters. Hover on your `ChatOllama(...)`\n",
    "# class to view the latest available supported parameters\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "system_prompt = (\"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# using LangChain Expressive Language chain syntax\n",
    "# learn more about the LCEL on\n",
    "# /docs/concepts/#langchain-expression-language-lcel\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# for brevity, response is printed in terminal\n",
    "# You can use LangServe to deploy your application for\n",
    "# production\n",
    "#print(chain.invoke({\"topic\": \"Space travel\"}))\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"Was macht GPT4 besser als GPT3 und GPT3.5?\"})\n",
    "#response = rag_chain.invoke({\"input\": \"Wie ist das Wetter heute?\"})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb25b1-02b7-43c4-9edc-24ac3aeb4716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
