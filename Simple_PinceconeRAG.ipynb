{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe1c8d7-a662-4167-ae6d-5f0959402f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"]=\"7c6028ed-8d2a-4003-8e7f-fca445aad9a1\"\n",
    "api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64348963-a050-4279-ad47-c5247719507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF gpt technical report and split\n",
    "loader = PyPDFLoader(\"gpt-4.pdf\")\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=550,\n",
    "                                                   chunk_overlap=55)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e5d671-c8be-4907-9231-08c11b5e86c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chringo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the embedding model \n",
    "model_name=\"BAAI/bge-m3\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bd59039-66e3-4431-a309-7434247db735",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'langchain-retrieval'\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024,\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b4fd8a-143c-4b71-aa0c-04aea91d7de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 8192, 'do_lower_case': False}) with Transformer model: XLMRobertaModel \n",
       "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='BAAI/bge-m3', cache_folder=None, model_kwargs={'device': 'cpu'}, encode_kwargs={'normalize_embeddings': False}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0d1fbd-5f53-47b2-a1ca-e2888493c174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d905f814-b274-4c3c-989b-fa7c5ad3a3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {},\n",
      " 'total_vector_count': 0}\n"
     ]
    }
   ],
   "source": [
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16b2281-0eb7-4932-b844-c6defa4322fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Pinecone index and insert the chunked docs as contents\n",
    "\n",
    "gptDB = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61fe273d-20ad-4185-85a7-dd11cc19da1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 1024,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 610}},\n",
      " 'total_vector_count': 610}\n"
     ]
    }
   ],
   "source": [
    "print(pc.Index(index_name).describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7567f4a4-28fe-4390-ac71-1cdd986f5fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.674135506, 'content': 'gpt-4 (no vision)\\ngpt3.5Figure 4. GPT performance on academic and professional exams. In each case, we simulate the\\nconditions and scoring of the real exam. Exams are ordered from low to high based on GPT-3.5\\nperformance. GPT-4 outperforms GPT-3.5 on most exams tested. To be conservative we report the\\nlower end of the range of percentiles, but this creates some artifacts on the AP exams which have very\\nwide scoring bins. For example although GPT-4 attains the highest possible score on AP Biology (5/5),', 'metadata': {'page': 5.0, 'source': 'gpt-4.pdf'}} \n",
      "\n",
      "{'score': 0.669958115, 'content': 'Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98', 'metadata': {'page': 97.0, 'source': 'gpt-4.pdf'}} \n",
      "\n",
      "{'score': 0.659658134, 'content': 'Askell et al.\\n2022Askell et al.\\n2022gpt-3.5-base gpt-3.5-base gpt-3.5-turbo gpt-4-base gpt-4-base gpt-4\\n0%10%20%30%40%50%60%70%\\nModelAccuracyAccuracy on adversarial questions (TruthfulQA mc1)\\nAnthropic-LM\\ngpt-3.5\\ngpt-4Figure 8: Performance of GPT-4 on TruthfulQA. Accuracy is shown on the y-axis, higher is better.\\nWe compare GPT-4 under zero-shot prompting, few-shot prompting, and after RLHF ﬁne-tuning.\\nGPT-4 signiﬁcantly outperforms both GPT-3.5 and Askell et al [100].ﬁxes to plot legend and title\\n65', 'metadata': {'page': 64.0, 'source': 'gpt-4.pdf'}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How is the performance of GPT-4 compared to GPT-3 or GPT-3.5?\"\n",
    "docs = gptDB.similarity_search_with_score(query, k=3)\n",
    "for i in docs:\n",
    "    doc, score = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata },\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10d4ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.689277887, 'content': 'Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98', 'metadata': {'page': 97.0, 'source': 'gpt-4.pdf'}} \n",
      "\n",
      "{'score': 0.660343409, 'content': '2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our', 'metadata': {'page': 43.0, 'source': 'gpt-4.pdf'}} \n",
      "\n",
      "{'score': 0.644460142, 'content': 'GPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business0%20%40%60%80%\\nCategoryAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4', 'metadata': {'page': 9.0, 'source': 'gpt-4.pdf'}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Wie hat sich die Performance von GPT-4 im Vergleich zu GPT-3 ode GPT3.5 verändert?\"\n",
    "docs = gptDB.similarity_search_with_score(query, k=3)\n",
    "for i in docs:\n",
    "    doc, score = i\n",
    "    print({\"score\": score, \"content\": doc.page_content, \"metadata\": doc.metadata },\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e095aaf-3d5b-44a8-b4bb-abd83f04d26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = gptDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a2ed7d-798b-4286-9f35-99719594e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 97.0, 'source': 'gpt-4.pdf'}, page_content='Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98'),\n",
       " Document(metadata={'page': 43.0, 'source': 'gpt-4.pdf'}, page_content='2 GPT-4 Observed Safety Challenges\\nGPT-4 demonstrates increased performance in areas such as reasoning, knowledge retention, and\\ncoding, compared to earlier models such as GPT-2[ 22] and GPT-3.[ 10] Many of these improvements\\nalso present new safety challenges, which we highlight in this section.\\nWe conducted a range of qualitative and quantitative evaluations of GPT-4. These evaluations\\nhelped us gain an understanding of GPT-4’s capabilities, limitations, and risks; prioritize our'),\n",
       " Document(metadata={'page': 9.0, 'source': 'gpt-4.pdf'}, page_content='GPT-4 signiﬁcantly reduces hallucinations relative to previous GPT-3.5 models (which have them-\\nselves been improving with continued iteration). GPT-4 scores 19 percentage points higher than our\\nlatest GPT-3.5 on our internal, adversarially-designed factuality evaluations (Figure 6).\\nlearning technology writing history math science recommendation code business0%20%40%60%80%\\nCategoryAccuracy\\nInternal factual eval by category\\nchatgpt-v2\\nchatgpt-v3\\nchatgpt-v4\\ngpt-4')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3781b08-ea2e-476e-8b57-dcc0c258c7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse import Langfuse\n",
    "\n",
    "langfuse_handler = CallbackHandler(\n",
    "  secret_key=\"sk-lf-1bd1db59-41de-49e5-896b-ee636349abd2\",\n",
    "  public_key=\"pk-lf-17015370-b722-4971-bc9b-898e2ff784bd\",\n",
    "  host=\"https://cloud.langfuse.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c5e49f-1077-4dce-bfc0-6d045761caca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the context, GPT-4 outperforms GPT-3.5 on most exams tested, and it significantly outperforms both GPT-3.5 and Askell et al [100] in terms of accuracy on adversarial questions (TruthfulQA mc1).\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "system_prompt = (\"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question.\"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Use four sentences maximum and keep the answer as precise as possible.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "response = rag_chain.invoke({\"input\": \"How is the performance of GPT-4 compared to GPT-3 or GPT-3.5?\"}, config={\"callbacks\": [langfuse_handler]})\n",
    "#response = rag_chain.invoke({\"input\": \"Wie ist das Wetter heute?\"})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bb25b1-02b7-43c4-9edc-24ac3aeb4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
